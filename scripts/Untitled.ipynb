{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rojinsafavi/anaconda/lib/python2.7/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from utils import project_folder\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BuildGraph():\n",
    "    \"\"\"Build a tensorflow network graph.\"\"\"\n",
    "    #TODO make it possible to change the prediciton function, cost and optimizer\n",
    "    #   by creating functions for each of those\n",
    "    def __init__(self, n_steps, n_input, n_classes, learning_rate, n_layers=1,\\\n",
    "    layer_sizes=tuple([128])):\n",
    "        self.x = tf.placeholder(\"float\", [None, n_steps, n_input], name='x')\n",
    "        self.y = tf.placeholder(\"float\", [None, n_classes], name='y')\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "        # Define weights and bias of last layer\n",
    "        weights = {\n",
    "            # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
    "            'out': tf.Variable(tf.random_normal([2*layer_sizes[-1], n_classes]))\n",
    "        }\n",
    "        biases = {\n",
    "            'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "        }\n",
    "\n",
    "        outputs = self.create_graph(n_layers, layer_sizes=layer_sizes, n_steps=n_steps, )\n",
    "\n",
    "        # Linear activation, using rnn inner loop last output\n",
    "        with tf.name_scope(\"predition\"):\n",
    "            pred = tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "            self.variable_summaries(pred)\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        with tf.name_scope(\"cost\"):\n",
    "            self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,\\\n",
    "             labels=self.y))\n",
    "            self.variable_summaries(self.cost)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(self.cost, \\\n",
    "         global_step=global_step)\n",
    "\n",
    "        # Evaluate model\n",
    "        with tf.name_scope(\"correct_pred\"):\n",
    "            correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(self.y, 1))\n",
    "            self.variable_summaries(self.cost)\n",
    "        with tf.name_scope(\"correct_pred\"):\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "            self.variable_summaries(self.accuracy)\n",
    "\n",
    "        self.merged_summaries = tf.summary.merge_all()\n",
    "        # Initializing the variables\n",
    "        self.init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "\n",
    "    def create_graph(self, num_layers, layer_sizes=tuple([128]), n_steps=1):\n",
    "        \"\"\"Create a graph using a functions for different layers\"\"\"\n",
    "        # TODO create a graph with variable number of nodes in each blstm layer\n",
    "        # Prepare data shape to match `bidirectional_rnn` function requirements\n",
    "        # Current data input shape: (batch_size, n_steps, n_input)\n",
    "        # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "        # Unstack to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "        assert len(layer_sizes) == num_layers\n",
    "        input1 = tf.unstack(self.x, n_steps, 1)\n",
    "        for number in range(num_layers):\n",
    "            input1 = self.blstm(input1, layer_name=\"layer\"+str(number),\\\n",
    "            n_hidden=layer_sizes[number])\n",
    "        return input1\n",
    "\n",
    "    @staticmethod\n",
    "    def blstm(input_vector, layer_name=\"layer1\", n_hidden=128, forget_bias=5.0):\n",
    "        \"\"\"Create a bidirectional LSTM using code from the example at\n",
    "         https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/bidirectional_rnn.py\"\"\"\n",
    "        # Define lstm cells with tensorflow\n",
    "        # Forward direction cell\n",
    "        lstm_fw_cell = rnn.LSTMCell(n_hidden, forget_bias=forget_bias)\n",
    "        # Backward direction cell\n",
    "        lstm_bw_cell = rnn.LSTMCell(n_hidden, forget_bias=forget_bias)\n",
    "        # try:\n",
    "        with tf.variable_scope(layer_name):\n",
    "            outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, \\\n",
    "            input_vector, dtype=tf.float32)\n",
    "        # except Exception: # Old TensorFlow version only returns outputs not states\n",
    "        #     with tf.variable_scope(layer_name):\n",
    "        #         outputs = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, \\\n",
    "        # x, dtype=tf.float32)\n",
    "        return outputs\n",
    "\n",
    "    @staticmethod\n",
    "    def variable_summaries(var):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\n",
    "        source: https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py\n",
    "        \"\"\"\n",
    "        with tf.name_scope('summaries'):\n",
    "            mean = tf.reduce_mean(var)\n",
    "            tf.summary.scalar('mean', mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "        tf.summary.scalar('stddev', stddev)\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '/Users/rojinsafavi/Desktop/nanopore-RNN/testing.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-746981453f4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-746981453f4d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproject_folder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/testing.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# grab labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/rojinsafavi/anaconda/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_pathlib_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '/Users/rojinsafavi/Desktop/nanopore-RNN/testing.npy'"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    \"\"\"Control the flow of the program\"\"\"\n",
    "    start = timer()\n",
    "    # load data\n",
    "    training = np.load(project_folder()+\"/testing.npy\")\n",
    "    # grab labels\n",
    "    labels = training[:, 1]\n",
    "    # grab feature vectors\n",
    "    features = training[:, 0]\n",
    "    # find the length of the label vector\n",
    "    label_len = len(labels[0])\n",
    "    feature_len = len(features[0])\n",
    "    # convert the inputs into numpy arrays\n",
    "    features2 = np.asarray([np.asarray(features[x]) for x in range(len(features))])\n",
    "    labels2 = np.asarray([np.asarray(labels[x]) for x in range(len(labels))])\n",
    "\n",
    "    # TODO make hyperparameters a json file\n",
    "    # Parameters\n",
    "    learning_rate = 0.001\n",
    "    training_iters = 10000\n",
    "    batch_size = 100\n",
    "    display_step = 10\n",
    "    # Network Parameters\n",
    "    n_input = feature_len\n",
    "    n_steps = 1 # one vector per timestep\n",
    "    layer_sizes = tuple([100, 100, 100, 100]) # hidden layer num of features\n",
    "    n_classes = label_len\n",
    "    n_layers = 4\n",
    "\n",
    "    # TODO build factoring by batch size into a method\n",
    "    # Should I make a data set class?\n",
    "    batch1_x = features2[:batch_size]\n",
    "    # batch1_x = batch1_x.reshape((batch_size, n_steps, n_input))\n",
    "    batch1_y = labels2[:batch_size]\n",
    "    batch1_x = batch1_x.reshape((batch_size, n_steps, n_input))\n",
    "\n",
    "    graph = BuildGraph(n_steps, n_input, n_classes, learning_rate, n_layers, layer_sizes)\n",
    "    x = graph.x\n",
    "    y = graph.y\n",
    "    cost = graph.cost\n",
    "    accuracy = graph.accuracy\n",
    "    merged_summaries = graph.merged_summaries\n",
    "    optimizer = graph.optimizer\n",
    "\n",
    "    init = graph.init\n",
    "    # Launch the graph\n",
    "    with tf.Session() as sess:\n",
    "        # TODO make directory for each new run\n",
    "        logfolder_path = os.path.join(project_folder(), 'logs/', datetime.now().strftime(\"%m%b-%d-%Hh-%Mm\"))\n",
    "        test_writer = tf.summary.FileWriter((logfolder_path), sess.graph)\n",
    "        sess.run(init)\n",
    "        step = 1\n",
    "        # Keep training until reach max iterations\n",
    "        while step * batch_size < training_iters:\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(optimizer, feed_dict={x: batch1_x, y: batch1_y})\n",
    "            if step % display_step == 0:\n",
    "                # Calculate batch accuracy\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                acc, summary = sess.run([accuracy, merged_summaries], \\\n",
    "                feed_dict={x: batch1_x, y: batch1_y}, run_metadata=run_metadata)\n",
    "\n",
    "                test_writer.add_summary(summary, step)\n",
    "                # Calculate batch loss\n",
    "                loss = sess.run(cost, feed_dict={x: batch1_x, y: batch1_y})\n",
    "                print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.5f}\".format(acc))\n",
    "            step += 1\n",
    "        print(\"Optimization Finished!\")\n",
    "        # Calculate accuracy for 128 mnist test images\n",
    "        # test_len = 128\n",
    "        print(\"Testing Accuracy: {}\".format(sess.run(accuracy, \\\n",
    "        feed_dict={x: batch1_x, y: batch1_y})))\n",
    "        test_writer.close()\n",
    "    stop = timer()\n",
    "    print(\"Running Time = {} seconds\".format(stop-start), file=sys.stderr)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    raise SystemExit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import generic_dna\n",
    "import glob\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_refrence_andEdit(referencePath, reference_Modified_Path):\n",
    "    '''Get fast5 file and remove \\n from the ends'''\n",
    "    with open(reference_Modified_Path, 'w') as outfile, open(referencePath, 'r') as infile:\n",
    "        for line in infile:\n",
    "            if \">\" in line:\n",
    "                outfile.write(line)\n",
    "            else:\n",
    "                T = line.rstrip()\n",
    "                outfile.write(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_refrence_andEdit(\"/Users/rojinsafavi/Desktop/signalAlign/tests/test_sequences/E.coli_K12.fasta\", \"/Users/rojinsafavi/Desktop/signalAlign/tests/test_sequences/E.coli_K12_modified.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_motif_complement(motif):\n",
    "    '''get the complement of a motif'''\n",
    "    dna = Seq(motif)\n",
    "    motif_complement = str(dna.complement())\n",
    "    return motif_complement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_Bed_file (reference_modified_Path, BED_file_path, motif1,modified_motif1,modified_motif1_comp, alphabet = \"E\" or \"X\", motif2 = False, modified_motif2 = False, modified_motif2_comp = False):\n",
    "    sequence_list = \"\"\n",
    "    seq_name = \"\"\n",
    "    string1 = motif1[[i for i in range(len(motif1)) if motif1[i] != modified_motif1[i]][0]]\n",
    "    motif1_comp = get_motif_complement(motif1)\n",
    "    with open(reference_modified_Path, 'r') as infile:\n",
    "        for line in infile:\n",
    "            if \">\" in line:\n",
    "                 seq_name = seq_name + line.rsplit()[0].split(\">\")[1]\n",
    "            else:\n",
    "                sequence_list = sequence_list + line\n",
    "    with open(BED_file_path, \"w\") as output:\n",
    "        motif1_replaced = sequence_list.replace(motif1, modified_motif1)\n",
    "        motif1_position = [m.start() for m in re.finditer('M', motif1_replaced)]\n",
    "        motif1_comp_replaced = sequence_list.replace(motif1_comp, modified_motif1_comp)\n",
    "        motif1_comp_position = [m.start() for m in re.finditer('M', motif1_comp_replaced)]\n",
    "        if motif2 == False:\n",
    "            for i in motif1_position:\n",
    "                output.write(seq_name + \"\\t\" + np.str(i) + \"\\t\" + \"+\" + \"\\t\" + string1 +\"\\t\" + alphabet + \"\\n\")\n",
    "            for i in motif1_comp_position:\n",
    "                output.write(seq_name + \"\\t\" + np.str(i) + \"\\t\" + \"-\" + \"\\t\" + string1 +\"\\t\" + alphabet + \"\\n\")\n",
    "        elif motif2 != False:\n",
    "            motif2_comp = get_motif_complement(motif2)\n",
    "            motif_1and2_replaced = motif1_replaced.replace(motif2, modified_motif2)\n",
    "            motif_1and2_positions = [m.start() for m in re.finditer('M', motif_1and2_replaced)]\n",
    "            motif_1and2_comp_replaced = motif1_comp_replaced.replace(motif2_comp, modified_motif2_comp)\n",
    "            motif_1and2_comp_positions = [m.start() for m in re.finditer('M', motif_1and2_comp_replaced)]\n",
    "            for i in motif_1and2_positions:\n",
    "                output.write(seq_name + \"\\t\" + np.str(i) + \"\\t\" + \"+\" + \"\\t\" + string1 +\"\\t\" + alphabet + \"\\n\")\n",
    "            for i in motif_1and2_comp_positions:\n",
    "                output.write(seq_name + \"\\t\" + np.str(i) + \"\\t\" + \"-\" + \"\\t\" + string1 +\"\\t\" + \"E\" + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_Bed_file(\"/Users/rojinsafavi/Desktop/signalAlign/tests/test_sequences/E.coli_K12_modified.fasta\",\"/Users/rojinsafavi/Desktop/signalAlign/tests/test_sequences/C-to_M.tsv\", \"CCAGG\", \"CMAGG\", \"GMTGG\", \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## Concatenate control and experimental assignments\n",
    "def concatenate_assignments (assignments_path1, assignments_path2, output):\n",
    "    '''concatenates control and experimental assignments'''\n",
    "    read_files = glob.glob(assignments_path1 + \"/*.assignments\") + glob.glob(assignments_path2 + \"/*.assignments\")\n",
    "    with open(output, \"w\") as outfile:\n",
    "        for f in read_files:\n",
    "            with open(f, \"rb\") as infile:\n",
    "                outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def func(a, b, c, *args):\n",
    "    for arg in args:\n",
    "        arg(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
